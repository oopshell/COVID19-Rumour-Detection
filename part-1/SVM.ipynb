{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('total_train_data.json'))\n",
    "train_labels = json.load(open('total_train_data_labels.json'))\n",
    "\n",
    "dev_data = json.load(open('total_dev_data.json'))\n",
    "dev_labels = json.load(open('total_dev_data_labels.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "def process_data(data, data_labels):\n",
    "    data_dict = {}\n",
    "    \n",
    "    tweet_ids = []\n",
    "    tweet_texts = []\n",
    "    tweet_retweets = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        tweet_text = clean_tweet(data.get(str(i)).get('source tweet text'))\n",
    "        \n",
    "        retweets = data.get(str(i)).get('retweets')\n",
    "        if retweets:\n",
    "            retweets = list(map(lambda x:clean_tweet(x), retweets))\n",
    "        \n",
    "        tweet_ids.append(data.get(str(i)).get('source tweet id'))\n",
    "        tweet_texts.append(tweet_text)\n",
    "        tweet_retweets.append(retweets)\n",
    "        \n",
    "    data_dict['tweet_ids'] = tweet_ids\n",
    "    data_dict['tweet_texts'] = tweet_texts\n",
    "    data_dict['tweet_retweets'] = tweet_retweets\n",
    "    \n",
    "    if not data_labels:\n",
    "        data_dict['tweet_labels'] = 2\n",
    "    else:\n",
    "        data_dict['tweet_labels'] = data_labels\n",
    "    \n",
    "    df = pd.DataFrame(data = data_dict)\n",
    "    return df\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # Remove hashtag while keeping hashtag text\n",
    "#     text = re.sub(r'#', '', text)\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    text = re.sub(r'\\&\\w*;', '', text)\n",
    "    # Remove tickers\n",
    "    text = re.sub(r'\\$\\w*', '', text)\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text)\n",
    "    # Remove URL, RT, mention(@)\n",
    "    text = re.sub(r'http(\\S)+', '', text)\n",
    "    text = re.sub(r'http ...', '', text)\n",
    "    text = re.sub(r'(RT|rt)[ ]*@[ ]*[\\S]+', '', text)\n",
    "    text = re.sub(r'RT[ ]?@', '', text)\n",
    "    text = re.sub(r'@[\\S]+', '', text)\n",
    "\n",
    "    text = re.sub(r'&amp;?', 'and', text)\n",
    "    text = re.sub(r'&lt;', '<', text)\n",
    "    text = re.sub(r'&gt;', '>', text)\n",
    "\n",
    "    # Remove emoji\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Remove redundent whitespace (including new line characters)\n",
    "    text = re.sub(r'\\s\\s+', '', text)\n",
    "    text = re.sub(r'[ ]{2, }', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = process_data(train_data,train_labels)\n",
    "df_dev = process_data(dev_data,dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>tweet_texts</th>\n",
       "      <th>tweet_retweets</th>\n",
       "      <th>tweet_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1250219300389974016</td>\n",
       "      <td>5. Can regularly rinsing your nose with saline...</td>\n",
       "      <td>[4. Can eating garlic help prevent infection w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>554886875303780352</td>\n",
       "      <td>French police chief killed himself after #Char...</td>\n",
       "      <td>[ How very sad., The trauma he must have faced...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237901309011021825</td>\n",
       "      <td>Coronavirus disease (COVID-19) advice for the ...</td>\n",
       "      <td>[Infection control for suspected or confirmed ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>524958128392376320</td>\n",
       "      <td>Ottawa police confirm that there were multiple...</td>\n",
       "      <td>[ Killers go berserk when cornered.Henceforth,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1239295488677085185</td>\n",
       "      <td>if the primary focus of a government isn't to ...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_ids                                        tweet_texts  \\\n",
       "0  1250219300389974016  5. Can regularly rinsing your nose with saline...   \n",
       "1   554886875303780352  French police chief killed himself after #Char...   \n",
       "2  1237901309011021825  Coronavirus disease (COVID-19) advice for the ...   \n",
       "3   524958128392376320  Ottawa police confirm that there were multiple...   \n",
       "4  1239295488677085185  if the primary focus of a government isn't to ...   \n",
       "\n",
       "                                      tweet_retweets  tweet_labels  \n",
       "0  [4. Can eating garlic help prevent infection w...             0  \n",
       "1  [ How very sad., The trauma he must have faced...             1  \n",
       "2  [Infection control for suspected or confirmed ...             0  \n",
       "3  [ Killers go berserk when cornered.Henceforth,...             0  \n",
       "4                                               None             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(data):\n",
    "    train_lst = []\n",
    "    for i in range(int(data.shape[0])):\n",
    "\n",
    "        tweets = data['tweet_texts'][i]\n",
    "        retweets = data['tweet_retweets'][i]\n",
    "        if type(retweets) == list:\n",
    "\n",
    "            for tweet in retweets:\n",
    "                tweets += tweet\n",
    "\n",
    "        train_lst.append(tweets)\n",
    "    return train_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lst = combine_text(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label(data):\n",
    "    label_lst = []\n",
    "    labels = data['tweet_labels']\n",
    "    for label in labels:\n",
    "        if label == 1:\n",
    "            label_lst.append('rumour')\n",
    "        else:\n",
    "            label_lst.append('nonrumour')\n",
    "    return label_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_lst = transform_label(df_train)\n",
    "# dev_labels = transform_label(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "530    0\n",
       "531    1\n",
       "532    0\n",
       "533    1\n",
       "534    0\n",
       "Name: tweet_labels, Length: 535, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_labels  = df_dev['tweet_labels']\n",
    "dev_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df = 2,max_df = 0.5,ngram_range = (1,2))\n",
    "tfidf = tfidf_vectorizer.fit_transform(train_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tfidf\n",
    "smote = SMOTE(random_state = 402)\n",
    "X_smote_tfidf, Y_smote_tfidf = smote.fit_resample(tfidf,label_lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########using SMOTE \n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\", decode_error=\"ignore\")\n",
    "count_vector = count_vectorizer.fit_transform(train_lst)\n",
    "smote = SMOTE(random_state = 402)\n",
    "X_smote, Y_smote = smote.fit_resample(count_vector,label_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_clf=Pipeline([\n",
    "                (\"tfidf\",TfidfTransformer(use_idf=True)),\n",
    "                (\"clf\", svm.SVC(kernel='linear', gamma=0.7, C=1.0))])\n",
    "\n",
    "\n",
    "\n",
    "SVMparameters={\n",
    "    \"clf__gamma\":[0.6,0.7,0.5,0.4,0.2],\n",
    "    \"clf__C\":[0.5,0.7,1.0,2.0,3.0]\n",
    "}\n",
    "SVM_clf_text = GridSearchCV(SVM_clf,SVMparameters, cv = 5)\n",
    "SVM_clf_text = SVM_clf_text.fit(X_smote, Y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9899598393574296"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_clf_text.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dev_count_vector = count_vectorizer.transform(dev_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8523364485981308"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_results = svm_clf.predict(dev_count_vector)\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "dev_f1 = f1_score(predict_results,dev_labels,average='micro')\n",
    "dev_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count_vector = count_vectorizer.transform(test_lst)\n",
    "test_predict_results = svm_clf.predict(test_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_result(predict_results):\n",
    "    final_result = []\n",
    "    for result in predict_results:\n",
    "        if result == 'rumour':\n",
    "            final_result.append(1)\n",
    "        else:\n",
    "            final_result.append(0)\n",
    "\n",
    "    index = [i for i in range(len(final_result))]\n",
    "\n",
    "    d = {}\n",
    "    d['Id'] = index\n",
    "    d['Predicted']=final_result\n",
    "\n",
    "    result_df = pd.DataFrame(d)\n",
    "    result_df\n",
    "\n",
    "    result_df.to_csv(r'/Users/lingyiqing/学习资料/IT/2022 s1/NLP/group assignment/project-data/test_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_result(test_predict_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM with countervect\n",
    "# SVM_clf=Pipeline([(\"vect\", CountVectorizer(stop_words=\"english\", decode_error=\"ignore\")), \n",
    "#                 (\"tfidf\",TfidfTransformer(use_idf=True)),\n",
    "#                 (\"clf\", svm.SVC(kernel='linear', gamma=0.7, C=1.0))])\n",
    "\n",
    "\n",
    "\n",
    "# SVMparameters={\n",
    "#     \"clf__gamma\":[0.6,0.7,0.5,0.4,0.2],\n",
    "#     \"clf__C\":[0.5,0.7,1.0,2.0,3.0]\n",
    "# }\n",
    "# SVM_clf_text = GridSearchCV(SVM_clf,SVMparameters, cv = 5)\n",
    "# SVM_clf_text = SVM_clf_text.fit(X_smote_tfidf, Y_smote_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM with countervect\n",
    "# SVM_clf=Pipeline([(\"vect\", CountVectorizer(stop_words=\"english\", decode_error=\"ignore\")), \n",
    "#                 (\"tfidf\",TfidfTransformer(use_idf=True)),\n",
    "#                 (\"clf\", svm.SVC(kernel='linear', gamma=0.7, C=1.0))])\n",
    "\n",
    "\n",
    "SVM_clf=Pipeline([(\"clf\", svm.SVC(kernel='linear', gamma=0.7, C=1.0))])\n",
    "\n",
    "SVMparameters={\n",
    "    \"clf__gamma\":[0.6,0.7,0.5,0.4,0.2],\n",
    "    \"clf__C\":[0.5,0.7,1.0,2.0,3.0]\n",
    "}\n",
    "SVM_clf_text = GridSearchCV(SVM_clf,SVMparameters, cv = 5)\n",
    "SVM_clf_text = SVM_clf_text.fit(X_smote_tfidf, Y_smote_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9899598393574296"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_clf_text.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dev_lst = combine_text(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9512467977264509"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_tfidf = tfidf_vectorizer.transform(dev_lst)\n",
    "predict_dev_tfidf = SVM_clf_text.predict(dev_tfidf)\n",
    "\n",
    "predict_result_tfidf = []\n",
    "for i in predict_dev_tfidf:\n",
    "    if i =='rumour':\n",
    "        predict_result_tfidf.append(1)\n",
    "    else:\n",
    "        predict_result_tfidf.append(0)\n",
    "        \n",
    "dev_f1_tfidf = f1_score(predict_result_tfidf,dev_labels,average='weighted')\n",
    "dev_f1_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "p, r, f, _ = precision_recall_fscore_support(dev_labels, predict_result_tfidf, pos_label=1, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8720379146919433"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using tfidf+SMOTE+SVM on test set data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = tfidf_vectorizer.transform(test_lst)\n",
    "predict_test_tfidf = SVM_clf_text.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_result(predict_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vect = CountVectorizer()\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_test_counts = count_vect.transform(dev_lst)\n",
    "# X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "530    0\n",
       "531    1\n",
       "532    0\n",
       "533    1\n",
       "534    0\n",
       "Name: tweet_labels, Length: 535, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21495327102803738"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline \n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "predict_results = [1] * len(dev_lst)\n",
    "dev_f1 = f1_score(predict_results,dev_labels,average='micro')\n",
    "dev_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# load dataset\n",
    "test_data = json.load(open('total_test_data.json'))\n",
    "df_test = process_data(test_data,None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source tweet id': '668849913678209024',\n",
       " 'source tweet text': 'That Superman poster is legit:\\n\\nhttps://t.co/fgnhH9kcJY https://t.co/dmikEp0igi',\n",
       " 'retweets': ['@snopes *ahem* So is this one. https://t.co/8cPWDOQgGs',\n",
       "  '@darksaber2k @snopes drops mic.... Walks out.',\n",
       "  '@snopes @CalBear949 https://t.co/Y87YHn3MPS has the old superman radio series. At least 1 is anti white supremacist: play nice with others',\n",
       "  '@snopes @mattkummer Well, Superman IS an illegal alien.',\n",
       "  '@snopes @comex mnyhhyyb']}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['213']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_ids</th>\n",
       "      <th>tweet_texts</th>\n",
       "      <th>tweet_retweets</th>\n",
       "      <th>tweet_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>555072815154475008</td>\n",
       "      <td>69 people die after drinking beer believed to ...</td>\n",
       "      <td>[Crossing Mozambique off my travel list, check...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>629503919098429440</td>\n",
       "      <td>Chick-Fil-A to open on Sundays</td>\n",
       "      <td>[ NOT COOL MAN, ]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1229732608889802753</td>\n",
       "      <td>Q: What can I do to protect myself from #COVID...</td>\n",
       "      <td>[The question which cannot be answered is w… ,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>489836441120145408</td>\n",
       "      <td>First pics from the site in Ukraine where #MH1...</td>\n",
       "      <td>[ photo courtesy?, Some may disagree with Maha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1240570885662289920</td>\n",
       "      <td>Should I wear a mask to protect myself from th...</td>\n",
       "      <td>[Can masks protect against the #COVID19infecti...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1234884616479051777</td>\n",
       "      <td>Can the virus that causes COVID-19 be transmit...</td>\n",
       "      <td>[How does COVID-19 spread? , Can CoVID-19 be c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1244004581010550785</td>\n",
       "      <td>Unter Überschrift \"Can CoVID-19 be caught from...</td>\n",
       "      <td>[Ja, die Menschen halten sich eben ganz oft ni...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1248902780556693506</td>\n",
       "      <td>Wondering what is COVID-19 Hotspot? Watch this...</td>\n",
       "      <td>[physical lock down will be released, as much ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1249529725019738113</td>\n",
       "      <td>What is COVID-19 status on 12.04.2020 ????</td>\n",
       "      <td>[Happy to share that one Covid positive case o...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1248769432748466177</td>\n",
       "      <td>Are antibiotics effective in preventing and tr...</td>\n",
       "      <td>[Can spraying alcohol or chlorine all over you...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_ids                                        tweet_texts  \\\n",
       "10   555072815154475008  69 people die after drinking beer believed to ...   \n",
       "11   629503919098429440                    Chick-Fil-A to open on Sundays    \n",
       "12  1229732608889802753  Q: What can I do to protect myself from #COVID...   \n",
       "13   489836441120145408  First pics from the site in Ukraine where #MH1...   \n",
       "14  1240570885662289920  Should I wear a mask to protect myself from th...   \n",
       "15  1234884616479051777  Can the virus that causes COVID-19 be transmit...   \n",
       "16  1244004581010550785  Unter Überschrift \"Can CoVID-19 be caught from...   \n",
       "17  1248902780556693506  Wondering what is COVID-19 Hotspot? Watch this...   \n",
       "18  1249529725019738113         What is COVID-19 status on 12.04.2020 ????   \n",
       "19  1248769432748466177  Are antibiotics effective in preventing and tr...   \n",
       "\n",
       "                                       tweet_retweets  tweet_labels  \n",
       "10  [Crossing Mozambique off my travel list, check...             2  \n",
       "11                                  [ NOT COOL MAN, ]             2  \n",
       "12  [The question which cannot be answered is w… ,...             2  \n",
       "13  [ photo courtesy?, Some may disagree with Maha...             2  \n",
       "14  [Can masks protect against the #COVID19infecti...             2  \n",
       "15  [How does COVID-19 spread? , Can CoVID-19 be c...             2  \n",
       "16  [Ja, die Menschen halten sich eben ganz oft ni...             2  \n",
       "17  [physical lock down will be released, as much ...             2  \n",
       "18  [Happy to share that one Covid positive case o...             2  \n",
       "19  [Can spraying alcohol or chlorine all over you...             2  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst = combine_text(df_test)\n",
    "test_labels = transform_label(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'rumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'rumour', 'rumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'rumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'rumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'rumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'rumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'rumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'rumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'rumour', 'nonrumour', 'rumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'rumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour'], dtype='<U9')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_results = SVM_clf_text.predict(test_lst)\n",
    "predict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "import collections\n",
    "l = len(dev_lst)\n",
    "index_lst = [i for i in range(l)]\n",
    "n = int(l*0.2)\n",
    "rumour_index_lst = sample(index_lst,  n)\n",
    "print(n)\n",
    "predict_results = []\n",
    "for i in range(l):\n",
    "    if i in rumour_index_lst:\n",
    "        predict_results.append(1)\n",
    "    else:\n",
    "        predict_results.append(0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_results = [1]*len(dev_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "p, r, f, _ = precision_recall_fscore_support(dev_labels, predict_results, pos_label=1, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35384615384615387"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9097976570820021"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################\n",
    "# naive bayes\n",
    "NB_clf=Pipeline([(\"vect\", CountVectorizer(stop_words=\"english\", decode_error=\"ignore\")), \n",
    "                (\"tfidf\",TfidfTransformer()),\n",
    "                (\"clf\", MultinomialNB())])\n",
    "NBparameters={\n",
    "     'tfidf__use_idf':(True,False),\n",
    "    'clf__alpha':[0,0.001,0.01,0.1,0.2],\n",
    "    'clf__fit_prior':[True,False]\n",
    "}\n",
    "NB_clf = GridSearchCV(NB_clf,NBparameters, cv = 5, n_jobs=-1)\n",
    "NB_clf=NB_clf.fit(train_lst, label_lst)\n",
    "NB_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9177570093457944"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_results = NB_clf.predict(dev_lst)\n",
    "dev_f1 = f1_score(predict_results,dev_labels,average='micro')\n",
    "dev_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'rumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'rumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'rumour', 'rumour', 'rumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'rumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'rumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'rumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'rumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'rumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'rumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'rumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour', 'nonrumour',\n",
       "       'nonrumour', 'nonrumour', 'nonrumour', 'rumour', 'nonrumour',\n",
       "       'nonrumour', 'rumour', 'nonrumour', 'nonrumour', 'rumour',\n",
       "       'nonrumour', 'nonrumour'], dtype='<U9')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_results = NB_clf.predict(test_lst)\n",
    "predict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "df1 = pd.read_csv('/Users/lingyiqing/Downloads/project data/1.csv')\n",
    "df2 = pd.read_csv('/Users/lingyiqing/Downloads/project data/2.csv')\n",
    "df3 = pd.read_csv('/Users/lingyiqing/Downloads/project data/3.csv')\n",
    "df4 = pd.read_csv('/Users/lingyiqing/Downloads/project data/4.csv')\n",
    "df5 = pd.read_csv('/Users/lingyiqing/Downloads/project data/5.csv')\n",
    "df6 = pd.read_csv('/Users/lingyiqing/Downloads/project data/6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>555</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>557</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  Predicted\n",
       "0      0          0\n",
       "1      1          1\n",
       "2      2          0\n",
       "3      3          0\n",
       "4      4          0\n",
       "..   ...        ...\n",
       "553  553          0\n",
       "554  554          0\n",
       "555  555          1\n",
       "556  556          0\n",
       "557  557          0\n",
       "\n",
       "[558 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [df1, df2, df3, df4, df5, df6]\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataset(dataset1, dataset2, dic):\n",
    "    predict1 = dataset1['Predicted']\n",
    "    predict2 = dataset2['Predicted']\n",
    "    \n",
    "    for i in range(len(predict1)):\n",
    "        if predict1[i] != predict2[i]:\n",
    "            dic[i].append(predict2[i])\n",
    "    return dic \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
