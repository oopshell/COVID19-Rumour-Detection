{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1651636876450,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "yifxIY-hKDlY"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1651636882354,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "ffrtF0pQKDlc"
   },
   "outputs": [],
   "source": [
    "train_data = json.load(open('total_train_data.json'))\n",
    "train_labels = json.load(open('total_train_data_labels.json'))\n",
    "\n",
    "dev_data = json.load(open('total_dev_data.json'))\n",
    "dev_labels = json.load(open('total_dev_data_labels.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1651636890152,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "Urc1Lm_XKDlf"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "def process_data(data, data_labels):\n",
    "    data_dict = {}\n",
    "    \n",
    "    tweet_ids = []\n",
    "    tweet_texts = []\n",
    "    tweet_retweets = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        tweet_text = clean_tweet(data.get(str(i)).get('source tweet text'))\n",
    "        \n",
    "        retweets = data.get(str(i)).get('retweets')\n",
    "        if retweets:\n",
    "          retweets = list(map(lambda x:clean_tweet(x), retweets))\n",
    "        \n",
    "        tweet_ids.append(data.get(str(i)).get('source tweet id'))\n",
    "        tweet_texts.append(tweet_text)\n",
    "        tweet_retweets.append(retweets)\n",
    "        \n",
    "    data_dict['tweet_ids'] = tweet_ids\n",
    "    data_dict['tweet_texts'] = tweet_texts\n",
    "    data_dict['tweet_retweets'] = tweet_retweets\n",
    "    \n",
    "    if not data_labels:\n",
    "      data_dict['tweet_labels'] = 2\n",
    "    else:\n",
    "      data_dict['tweet_labels'] = data_labels\n",
    "    \n",
    "    df = pd.DataFrame(data = data_dict)\n",
    "    return df\n",
    "\n",
    "def clean_tweet(text):\n",
    "  # Remove hashtag while keeping hashtag text\n",
    "  text = re.sub(r'#', '', text)\n",
    "  # Remove HTML special entities (e.g. &amp;)\n",
    "  text = re.sub(r'\\&\\w*;', '', text)\n",
    "  # Remove tickers\n",
    "  text = re.sub(r'\\$\\w*', '', text)\n",
    "  # Remove hyperlinks\n",
    "  text = re.sub(r'https?:\\/\\/.*\\/\\w*', '', text)\n",
    "  # Remove URL, RT, mention(@)\n",
    "  text = re.sub(r'http(\\S)+', '', text)\n",
    "  text = re.sub(r'http ...', '', text)\n",
    "  text = re.sub(r'(RT|rt)[ ]*@[ ]*[\\S]+', '', text)\n",
    "  text = re.sub(r'RT[ ]?@', '', text)\n",
    "  text = re.sub(r'@[\\S]+', '', text)\n",
    "  \n",
    "  text = re.sub(r'&amp;?', 'and', text)\n",
    "  text = re.sub(r'&lt;', '<', text)\n",
    "  text = re.sub(r'&gt;', '>', text)\n",
    "  \n",
    "  # Remove emoji\n",
    "  text = emoji.demojize(text)\n",
    "  \n",
    "  # Remove redundent whitespace (including new line characters)\n",
    "  text = text.replace('\\n', ' ')\n",
    "  text = re.sub(r'\\s\\s+', '', text)\n",
    "  text = re.sub(r'[ ]{2, }', ' ', text)\n",
    "    \n",
    "  return text\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1651636894092,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "be1pXd1WKDlh"
   },
   "outputs": [],
   "source": [
    "df_train = process_data(train_data,train_labels)\n",
    "df_dev = process_data(dev_data,dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2679,
     "status": "ok",
     "timestamp": 1651636911226,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "jBtFX8ylKDli",
    "outputId": "cf37627b-d111-461c-b81e-b0dd432d31f7"
   },
   "outputs": [],
   "source": [
    "#load pretrained bert base model\n",
    "#this is already trained on a large courpus\n",
    "from transformers import BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"Done loading BERT model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1651637426478,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "5yY-xf0QKDlk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, maxlen): \n",
    "\n",
    "        #Store the contents of the file in a pandas dataframe\n",
    "        self.df = dataframe\n",
    "\n",
    "        #Initialize the BERT tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)     \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        tweet_id = self.df.loc[index, 'tweet_ids']\n",
    "        tweet_text = self.df.loc[index, 'tweet_texts']\n",
    "        tweet_retweet = self.df.loc[index, 'tweet_retweets']\n",
    "        tweet_labels = self.df.loc[index, 'tweet_labels']\n",
    "        \n",
    "        retweet_text = \"\"\n",
    "        if(tweet_retweet):\n",
    "            random.shuffle(tweet_retweet)\n",
    "            retweet_text = '[SEP]'.join(tweet_retweet)\n",
    "            \n",
    "        #Preprocessing the text to be suitable for BERT\n",
    "        tokens = self.tokenizer(tweet_text,\n",
    "                                retweet_text,\n",
    "                                truncation=True,\n",
    "                                padding='max_length',\n",
    "                                max_length=self.maxlen) #Tokenize the sentence\n",
    "        \n",
    "        tokens_ids_tensor = torch.tensor(tokens['input_ids']) #Converting the list to a pytorch tensor\n",
    "\n",
    "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = torch.tensor(tokens['attention_mask'])\n",
    "\n",
    "        seg_ids = torch.tensor(tokens['token_type_ids'])\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask, seg_ids, tweet_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3530,
     "status": "ok",
     "timestamp": 1651637435958,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "5Ovp_8SoKDlm",
    "outputId": "596203b0-cea1-4570-d5ba-9698580ec767"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Creating instances of training and development set\n",
    "#maxlen sets the maximum length a sentence can have\n",
    "#any sentence longer than this length is truncated to the maxlen size\n",
    "train_set = TweetDataset(dataframe = df_train, maxlen = 512) \n",
    "dev_set = TweetDataset(dataframe = df_dev, maxlen = 512)\n",
    "\n",
    "#Creating intsances of training and development dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size = 4)\n",
    "dev_loader = DataLoader(dev_set, batch_size = 4)\n",
    "\n",
    "print(\"Done preprocessing training and development data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1651637438986,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "sRHIpA9GKDlm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class RumourClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RumourClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        #Classification layer\n",
    "        #input dimension is 768 because [CLS] embedding has a dimension of 768\n",
    "        #output dimension is 1 because we're working with a binary classification problem\n",
    "        self.cls_layer = nn.Linear(768, 1) #initialize the layer\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg_ids):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, token_type_ids = seg_ids)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0] #for all the context, just take the first cls token\n",
    "\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3063,
     "status": "ok",
     "timestamp": 1651637444142,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "ZdmGi-jGKDln",
    "outputId": "7b605a72-b5ba-4581-9b9e-438eb8c58540"
   },
   "outputs": [],
   "source": [
    "gpu = 0 #gpu ID\n",
    "\n",
    "print(\"Creating the sentiment classifier, initialised with pretrained BERT-BASE parameters...\")\n",
    "net = RumourClassifier() #initailize the net\n",
    "net.cuda(gpu) #Enable gpu support for the model\n",
    "print(\"Done creating the sentiment classifier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1651637445311,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "dRqZCBfyKDlo"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opti = optim.Adam(net.parameters(), lr = 2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1651637447365,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "KEQXHorxKDlo"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "import numpy as np\n",
    "\n",
    "def eval_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    p = precision_score(y_true, y_pred, average=\"binary\")\n",
    "    r = recall_score(y_true, y_pred, average=\"binary\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"binary\")\n",
    "\n",
    "    return (acc, p, r, f1)\n",
    "\n",
    "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    best_acc=0\n",
    "    best_f1 = 0\n",
    "    \n",
    "    st = time.time()\n",
    "    \n",
    "    for ep in range(max_eps):\n",
    "        loss_list = []\n",
    "        pred_list = []\n",
    "        true_list = []\n",
    "        for it, (seq, attn_masks, seg_ids, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad() #make all the gradient zero  \n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, seg_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu), labels.cuda(gpu)\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks, seg_ids)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step() #update the weight with the gradient\n",
    "            \n",
    "            loss_list.append(loss.item())\n",
    "            pred_list.append( (torch.sigmoid(logits) > 0.5).long().squeeze().cpu().numpy())\n",
    "            true_list.append(labels.cpu().numpy())\n",
    "              \n",
    "            if it % 100 == 0:\n",
    "                y_pred = np.concatenate(pred_list)\n",
    "                y_true = np.concatenate(true_list)\n",
    "                loss = np.mean(loss_list)\n",
    "                acc, p, r, f1 = eval_metrics(y_true, y_pred)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; acc: {}; f1: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc,f1, (time.time()-st)))\n",
    "                st = time.time()\n",
    "                loss_list = []\n",
    "                pred_list = []\n",
    "                true_list = []\n",
    "\n",
    "        dev_acc, dev_f1, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
    "        \n",
    "        print(\"Epoch {} complete! Development acc: {}; Development f1: {}; Development Loss: {}\".format(ep, dev_acc, dev_f1 ,dev_loss))\n",
    "        \n",
    "        if dev_acc > best_acc:\n",
    "            print(\"Best development acc improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "            best_acc = dev_acc\n",
    "            torch.save(net.state_dict(), 'bert_epoch{}_acc_{}.dat'.format(ep, best_acc))\n",
    "        if dev_f1 > best_f1:\n",
    "            print(\"Best development f1 improved from {} to {}, saving model...\".format(best_f1, dev_f1))\n",
    "            best_f1 = dev_f1\n",
    "            torch.save(net.state_dict(), 'bert_epoch{}_f1_{}.dat'.format(ep, best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1651641618515,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "rNBIGpxIKDlp"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def predict(net, data_set, gpu):\n",
    "    data_loader = DataLoader(data_set, batch_size = 1)\n",
    "    net.eval()\n",
    "    predicted_dict = {}\n",
    "    \n",
    "    for it, (seq, attn_masks, seg_ids, labels) in enumerate(data_loader):\n",
    "        #Clear gradients\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            seq, attn_masks, seg_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu), labels.cuda(gpu)\n",
    "            \n",
    "            logits = net(seq, attn_masks, seg_ids)\n",
    "            \n",
    "            probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "            soft_probs = (probs > 0.5).long()\n",
    "            predictedLabel = int(soft_probs.item())\n",
    "            predicted_dict[it] = predictedLabel\n",
    "\n",
    "    return predicted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1651641566384,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "4q6ihihbKDlq"
   },
   "outputs": [],
   "source": [
    "def evaluate(net, criterion, dataloader, gpu):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        loss_list = []\n",
    "        pred_list = []\n",
    "        true_list = []\n",
    "        for seq, attn_masks, seg_ids, labels in dataloader:\n",
    "            seq, attn_masks, seg_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu), labels.cuda(gpu)\n",
    "            logits = net(seq, attn_masks, seg_ids)\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "            loss_list.append(loss.item())\n",
    "            pred_list.append( (torch.sigmoid(logits) > 0.5).long().squeeze().cpu().numpy())\n",
    "            true_list.append(labels.cpu().numpy())\n",
    "        y_pred = np.concatenate(pred_list)\n",
    "        y_true = np.concatenate(true_list)\n",
    "        loss = np.mean(loss_list)\n",
    "        acc, p, r, f1 = eval_metrics(y_true, y_pred)\n",
    "    return acc, f1,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3578468,
     "status": "error",
     "timestamp": 1651641484381,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "jXXqgROYKDlr",
    "outputId": "63782213-9a99-4c33-ef51-4d54c71accce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch =40\n",
    "\n",
    "#fine-tune the model\n",
    "train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)\n",
    "print(\"------------------- Finish -------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWcPxgfcHFu7"
   },
   "source": [
    "## Test prediction and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2990,
     "status": "ok",
     "timestamp": 1651641544099,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "UDtow3Cogdqn",
    "outputId": "9f591942-a826-4eb5-fcc7-e59819ff51a4"
   },
   "outputs": [],
   "source": [
    "# load existing model\n",
    "net = RumourClassifier()\n",
    "net.load_state_dict(torch.load('./bert_epoch3_f1_0.9700934579439252.dat'))\n",
    "net.cuda(gpu) #Enable gpu support for the model #tell the model to move to GPU\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 2133,
     "status": "ok",
     "timestamp": 1651641579135,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "l2Ly0f93eYx5"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "test_data = json.load(open('total_test_data.json'))\n",
    "df_test = process_data(test_data,None)\n",
    "test_set = TweetDataset(df_test, maxlen = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53521,
     "status": "ok",
     "timestamp": 1651641676377,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "UKuWkHBMUVTN",
    "outputId": "996230bc-448d-4f5c-eadf-0a1e4ba2b636"
   },
   "outputs": [],
   "source": [
    "test_predicted_dict = predict(net, test_set ,gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1651641700294,
     "user": {
      "displayName": "Han Xiao",
      "userId": "17173722672069937808"
     },
     "user_tz": -600
    },
    "id": "MS3JL-D3W_eC"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('test.csv', 'w') as f:\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "    for key in test_predicted_dict.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,test_predicted_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covid tweet prediction and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RumourClassifier()\n",
    "net.load_state_dict(torch.load('bert_epoch3_f1_0.9700934579439252.dat'))\n",
    "net.cuda(gpu) #Enable gpu support for the model #tell the model to move to GPU\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_data = json.load(open('total_covid_data.json'))\n",
    "df_covid = process_data(covid_data,None)\n",
    "covid_set = TweetDataset(df_covid, maxlen = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "covid_predicted_dict = predict(net, covid_set ,gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_prediction = {}\n",
    "for key in covid_predicted_dict.keys():\n",
    "  prediction = {}\n",
    "  prediction['id'] = covid_data[str(key)].get('source tweet id')\n",
    "  prediction['prediction'] = covid_predicted_dict[key]\n",
    "  covid_prediction[key] = prediction\n",
    "\n",
    "with open('covid_prediction.json', 'w') as f:\n",
    "    json.dump(covid_prediction, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert4.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "bcc88a5c2b6accdcaf39c87a931cb715cc1ab684beb32819a99a5a377f971b8d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
